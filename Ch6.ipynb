{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Elman Neural Networks\n",
    "Elman神经网络是一种流行的部分递归神经网络。它们最初被设计为学习顺序或时间变化的模式，并已成功地用于模式分类、控制、优化。\n",
    "\n",
    "它们由一个输入层、一个上下文层（也叫循环层或延迟层，见图6.1）、一个隐藏层和一个输出层组成。每一层都包含一个或多个神经元，它们通过计算其输入的加权和的非线性函数，将信息从一层传播到另一层。\n",
    "\n",
    "在Elman神经网络中，上下文层的神经元数量等于隐藏层的神经元数量。此外，上下文层的神经元与隐藏层的所有神经元完全连接。\n",
    "\n",
    "类似于普通的前馈神经网络，神经元之间所有连接的强度由一个权重决定。最初，所有的权重值都是随机选择的，并在训练过程中进行优化。\n",
    "\n",
    "记忆是通过延迟（上下文）单元发生的，延迟单元由隐藏层神经元供给。从隐藏层到延迟单元的递归连接的权重固定为1，这导致延迟单元始终保持着隐藏单元先前值的副本。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<img src='Elman Neural Networks.png'> \n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "%%html\n",
    "<img src='Elman Neural Networks.png'> "
   ]
  },
  {
   "source": [
    "## Prepare You Data for Easy Use"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "loc= \"update_COE.csv\"\n",
    "temp = pd.read_csv(loc)\n",
    "data = temp.drop(temp.columns[[0,1]], axis=1)\n",
    "y=data['COE$']\n",
    "x=data.drop(data.columns[[0,4]], axis =1)#drop the first line\n",
    "x=x.apply(np.log)\n",
    "x=pd.concat([x, data['Open?']], axis =1)\n",
    "from sklearn import preprocessing\n",
    "scaler_x = preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "x = np.array(x).reshape(len(x),4)\n",
    "x = scaler_x.fit_transform(x)\n",
    "scaler_y = preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "y = np.array(y).reshape(len(y),1)\n",
    "y = np.log(y)\n",
    "y = scaler_y.fit_transform(y)\n",
    "y = y.tolist()\n",
    "x = x.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyneurgen.neuralnet import NeuralNet\n",
    "from pyneurgen.recurrent import ElmanSimpleRecurrent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'xrange' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-14776a64f73e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mhidden_nodes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0moutput_nodes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mfit1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_layers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_nodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mhidden_nodes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_nodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mElmanSimpleRecurrent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mfit1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandomize_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mfit1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_activation_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'sigmoid'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pyneurgen\\neuralnet.py\u001b[0m in \u001b[0;36minit_layers\u001b[1;34m(self, input_nodes, total_hidden_nodes_list, output_nodes, *recurrent_mods)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mrecurrent_mod\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrecurrent_mods\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m             \u001b[0mrecurrent_mod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mset_halt_on_extremes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhalt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pyneurgen\\recurrent.py\u001b[0m in \u001b[0;36mapply_config\u001b[1;34m(self, neural_net)\u001b[0m\n\u001b[0;32m     87\u001b[0m         \"\"\"\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneural_net\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneural_net\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pyneurgen\\recurrent.py\u001b[0m in \u001b[0;36m_apply_config\u001b[1;34m(self, neural_net)\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0msnode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_source_nodes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneural_net\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[0mprev_copy_node\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mlevel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy_levels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m                 \u001b[0mcopy_node\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCopyNode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlevel\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'xrange' is not defined"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(2020)\n",
    "fit1 = NeuralNet()\n",
    "input_nodes = 4\n",
    "hidden_nodes = 7\n",
    "output_nodes = 1\n",
    "fit1.init_layers(input_nodes, [hidden_nodes], output_nodes, ElmanSimpleRecurrent())\n",
    "fit1.randomize_network()\n",
    "fit1.layers[1].set_activation_type('sigmoid')\n",
    "fit1.set_learnrate(0.05)\n",
    "fit1.set_all_inputs(x)\n",
    "fit1.set_all_targets(y)"
   ]
  },
  {
   "source": [
    "在上面的代码中，隐藏节点使用了sigmoid激活函数，学习率设置为5%。这意味着在训练过程中，目标和输出的每个实例之间的5%的误差将被回传到网络中。\n",
    "\n",
    "## Exploring the Error Surface\n",
    "\n",
    "神经网络模型有很多权重，其数值必须是 被确定为产生一个最优解。输出为 输入的函数很可能是高度非线性的，这就意味着 使得优化过程变得复杂。寻找全局的 避免局部最小值的最优解是一个挑战，因为...。误差函数一般既不凸也不凹。这个 意味着所有二次偏导数的矩阵（通常为 称为黑森）既不是正半定式，也不是负半定型。这一观察的实际后果是，神经网络会卡在局部最小值，这取决于误差面的形状。\n",
    "\n",
    "为了使其类似于一元函数，请注意sin(x)既不是凸的也不是凹的。它有无限多的最大值和最小值，见图 6.4（上图）。而$x^2$只有一个最小值，$-x^2$只有一个最大值，见图6.4（下图）。这一观察的实际后果是，根据误差面的形状，神经网络可能会卡在局部最小值。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<img src='one variable.png'> \n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "%%html\n",
    "<img src='one variable.png'> "
   ]
  },
  {
   "source": [
    "如果你绘制神经网络误差作为权重的函数，你很可能会看到一个非常粗糙的表面，有许多局部最小值。如图6.5所示，它可以有非常多的峰和谷。它可能在某些方向上是高度弯曲的，而在其他方向上是平坦的。这使得优化过程非常复杂。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<img src='error surface.png'> \n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "%%html\n",
    "<img src='error surface.png'> "
   ]
  },
  {
   "source": [
    "## A Super Simple Way to Fit the Model"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = len(x)\n",
    "learn_end_point = int( length * 0.95)\n",
    "fit1.set_learn_range(0, learn_end_point)\n",
    "fit1.set_test_range(learn_end_point + 1, length -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit1.learn ( epochs=100, show_epoch_results=True, random_testing=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = fit1.test()"
   ]
  }
 ]
}